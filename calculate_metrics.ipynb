{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ec8c5278-c0cc-41b6-9bee-7214713fef0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import nibabel as nib\n",
    "import SimpleITK as sitk\n",
    "from scipy.spatial.distance import directed_hausdorff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "386bda1a-24c3-46fa-871b-19b4fd5a12a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the NIfTI files\n",
    "GTimage1 = sitk.ReadImage('ai4mi_project/data/segthor_train/train/Patient_07/GT_fixed.nii.gz')\n",
    "GTimage2 = sitk.ReadImage('ai4mi_project/data/segthor_train/train/Patient_17/GT_fixed.nii.gz')\n",
    "GTimage3 = sitk.ReadImage('ai4mi_project/data/segthor_train/train/Patient_18/GT_fixed.nii.gz')\n",
    "GTimage4 = sitk.ReadImage('ai4mi_project/data/segthor_train/train/Patient_34/GT_fixed.nii.gz')\n",
    "GTimage5 = sitk.ReadImage('ai4mi_project/data/segthor_train/train/Patient_39/GT_fixed.nii.gz')\n",
    "\n",
    "PTimage1 = sitk.ReadImage('ai4mi_project/data/segthor_train/train/ce_rotated_elastic/stitched_volumes/Patient_07.nii.gz')\n",
    "PTimage2 = sitk.ReadImage('ai4mi_project/data/segthor_train/train/ce_rotated_elastic/stitched_volumes/Patient_17.nii.gz')\n",
    "PTimage3 = sitk.ReadImage('ai4mi_project/data/segthor_train/train/ce_rotated_elastic/stitched_volumes/Patient_18.nii.gz')\n",
    "PTimage4 = sitk.ReadImage('ai4mi_project/data/segthor_train/train/ce_rotated_elastic/stitched_volumes/Patient_34.nii.gz')\n",
    "PTimage5 = sitk.ReadImage('ai4mi_project/data/segthor_train/train/ce_rotated_elastic/stitched_volumes/Patient_39.nii.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e2d2edc0-9906-4dc1-aaf2-4b9c29f91de7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load NIfTI images\n",
    "GTimage1_nii = nib.load('ai4mi_project/data/segthor_train/train/Patient_07/GT_fixed.nii.gz')\n",
    "GTimage2_nii = nib.load('ai4mi_project/data/segthor_train/train/Patient_17/GT_fixed.nii.gz')\n",
    "GTimage3_nii = nib.load('ai4mi_project/data/segthor_train/train/Patient_18/GT_fixed.nii.gz')\n",
    "GTimage4_nii = nib.load('ai4mi_project/data/segthor_train/train/Patient_34/GT_fixed.nii.gz')\n",
    "GTimage5_nii = nib.load('ai4mi_project/data/segthor_train/train/Patient_39/GT_fixed.nii.gz')\n",
    "\n",
    "PTimage1_nii = nib.load('ai4mi_project/data/segthor_train/train/ce_rotated_elastic/stitched_volumes/Patient_07.nii.gz')\n",
    "PTimage2_nii = nib.load('ai4mi_project/data/segthor_train/train/ce_rotated_elastic/stitched_volumes/Patient_17.nii.gz')\n",
    "PTimage3_nii = nib.load('ai4mi_project/data/segthor_train/train/ce_rotated_elastic/stitched_volumes/Patient_18.nii.gz')\n",
    "PTimage4_nii = nib.load('ai4mi_project/data/segthor_train/train/ce_rotated_elastic/stitched_volumes/Patient_34.nii.gz')\n",
    "PTimage5_nii = nib.load('ai4mi_project/data/segthor_train/train/ce_rotated_elastic/stitched_volumes/Patient_39.nii.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "089628d6-a1fc-4a9f-8272-8a5a43b4c33c",
   "metadata": {},
   "outputs": [],
   "source": [
    "GTS = [GTimage1, GTimage2, GTimage3, GTimage4, GTimage5]\n",
    "GTS_nii = [GTimage1_nii, GTimage2_nii, GTimage3_nii, GTimage4_nii, GTimage5_nii]\n",
    "\n",
    "PTS = [PTimage1, PTimage2, PTimage3, PTimage4, PTimage5]\n",
    "PTS_nii = [PTimage1_nii, PTimage2_nii, PTimage3_nii, PTimage4_nii, PTimage5_nii]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69224d65-a8d0-4409-a0c9-8385c82e795a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_hausdorff(img1, img2):\n",
    "    \"\"\"\n",
    "    img1 has to be the GROUND TRUTH !\n",
    "    \"\"\"\n",
    "\n",
    "    # Convert to numpy arrays\n",
    "    array1 = sitk.GetArrayFromImage(img1)\n",
    "    array2 = sitk.GetArrayFromImage(img2)\n",
    "\n",
    "    # Get all unique labels \n",
    "    labels1 = np.unique(array1)\n",
    "    labels2 = np.unique(array2)\n",
    "    \n",
    "    # Dictionary to store Hausdorff distance for each label\n",
    "    hausdorff_distances = {}\n",
    "\n",
    "    common_labels = labels1\n",
    "\n",
    "    # Loop over each label (segmentation)\n",
    "    for label in common_labels:\n",
    "        \n",
    "        if label == 0:  # Skip background\n",
    "            continue\n",
    "        \n",
    "        # Create binary masks for the current label in both images\n",
    "        mask1 = (array1 == label).astype(int)\n",
    "        \n",
    "        if label == 1:\n",
    "            mask2 = (array2 == labels2[1]).astype(int)\n",
    "        if label == 2:\n",
    "            mask2 = (array2 == labels2[2]).astype(int)\n",
    "        if label == 3:\n",
    "            mask2 = (array2 == labels2[3]).astype(int)\n",
    "        if label == 4:\n",
    "            mask2 = (array2 == labels2[4]).astype(int)\n",
    "        \n",
    "        # Get the coordinates of non-zero points in the binary masks\n",
    "        coords1 = np.column_stack(np.where(mask1))\n",
    "        coords2 = np.column_stack(np.where(mask2))\n",
    "\n",
    "        # Compute the directed Hausdorff distances\n",
    "        hausdorff_distance_1_to_2 = directed_hausdorff(coords1, coords2)[0]\n",
    "        hausdorff_distance_2_to_1 = directed_hausdorff(coords2, coords1)[0]\n",
    "    \n",
    "        # Symmetric Hausdorff distance (maximum of the two directed distances)\n",
    "        hausdorff_distance = max(hausdorff_distance_1_to_2, hausdorff_distance_2_to_1)\n",
    "    \n",
    "        # Store the result\n",
    "        hausdorff_distances[label] = hausdorff_distance\n",
    "    \n",
    "        # print(f\"Label {label}: Hausdorff Distance = {hausdorff_distance:.4f}\")\n",
    "\n",
    "    return np.mean(list(hausdorff_distances.values()))\n",
    "\n",
    "haus = []\n",
    "for gt, pt in zip(GTS, PTS):\n",
    "    haus.append(compute_hausdorff(gt, pt))\n",
    "\n",
    "print(\"\\nMean Hausdorff: \", np.mean(haus))\n",
    "print(\"\\nSTD Hausdorff: \", np.std(haus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dc712463-8bae-4ad5-98cc-64761ff942b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_volumetric(img1, img2):\n",
    "    \"\"\"\n",
    "    img1 has to be the GROUND TRUTH !\n",
    "    \"\"\"\n",
    "\n",
    "    # Get the data arrays\n",
    "    mask1 = img1.get_fdata()\n",
    "    mask2 = img2.get_fdata()\n",
    "    \n",
    "    # Get all unique labels from both masks (excluding 0 for the background)\n",
    "    labels_1 = np.unique(mask1[mask1 > 0])\n",
    "    labels_2 = np.unique(mask2[mask2 > 0])\n",
    "    \n",
    "    # Get the voxel sizes from the headers\n",
    "    voxel_size1 = np.prod(img1.header.get_zooms())  # in mm^3\n",
    "    voxel_size2 = np.prod(img2.header.get_zooms())  # in mm^3\n",
    "    \n",
    "    # Function to compute volume for a given label\n",
    "    def compute_volume(mask, label, voxel_size):\n",
    "        return np.sum(mask == label) * voxel_size\n",
    "    \n",
    "    # Iterate over each label (organ) and compute Volumetric Similarity\n",
    "    vs_results = {}\n",
    "    \n",
    "    for label in labels_1:\n",
    "        # Compute volumes for the current label in both scans\n",
    "        volume1 = compute_volume(mask1, label, voxel_size1)\n",
    "    \n",
    "        if label == 1:\n",
    "            volume2 = compute_volume(mask2, labels_2[0], voxel_size2)\n",
    "        if label == 2:\n",
    "            volume2 = compute_volume(mask2, labels_2[1], voxel_size2)\n",
    "        if label == 3:\n",
    "            volume2 = compute_volume(mask2, labels_2[2], voxel_size2)\n",
    "        if label == 4:\n",
    "            volume2 = compute_volume(mask2, labels_2[3], voxel_size2)\n",
    "    \n",
    "    \n",
    "        # Compute VS only if both volumes are non-zero\n",
    "        if volume1 + volume2 > 0:\n",
    "            vs = 1 - abs(volume1 - volume2) / (volume1 + volume2)\n",
    "        else:\n",
    "            vs = 0  # If both volumes are 0, there's no overlap\n",
    "    \n",
    "        # Store the result\n",
    "        vs_results[label] = vs\n",
    "\n",
    "        # print(f\"\\nLabel {label}\")\n",
    "        # print(f\"Volume img1: {volume1}\")\n",
    "        # print(f\"Volume img2: {volume2}\\n\")\n",
    "    \n",
    "    # Print Volumetric Similarity results for each organ\n",
    "    # for label, vs in vs_results.items():\n",
    "    #     print(f\"Label {label}: Volumetric Similarity = {vs:.4f}\")\n",
    "    # print(\"i\")\n",
    "    return np.mean(list(vs_results.values()))\n",
    "\n",
    "# vol = []\n",
    "# for gt, pt in zip(GTS_nii, PTS_nii):\n",
    "#     vol.append(compute_volumetric(gt, pt))\n",
    "\n",
    "# print(\"\\nMean volume: \", np.mean(vol))\n",
    "# print(\"\\nSTD volume: \", np.std(vol))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdce49a5-0248-46be-9462-9d9b44084f8f",
   "metadata": {},
   "source": [
    "# I have to compute the volumetric separetly because my kernel died"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0031383a-62aa-4760-9de9-cb2e831f4fa1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9802115767442802\n"
     ]
    }
   ],
   "source": [
    "vol1 = compute_volumetric(GTS_nii[0], PTS_nii[0])\n",
    "print(vol1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "17f93613-fdbf-43a7-9356-5735f9c7d706",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.893861309773059\n"
     ]
    }
   ],
   "source": [
    "vol2 = compute_volumetric(GTS_nii[1], PTS_nii[1])\n",
    "print(vol2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4b7fe753-129c-449a-a460-94f9fd5ae427",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9344621027255262\n"
     ]
    }
   ],
   "source": [
    "vol3 = compute_volumetric(GTS_nii[2], PTS_nii[2])\n",
    "print(vol3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "037f8f7e-9b5f-4460-95b6-e46c1c5aa043",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8351096432126663\n"
     ]
    }
   ],
   "source": [
    "vol4 = compute_volumetric(GTS_nii[3], PTS_nii[3])\n",
    "print(vol4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8c49cced-015e-4e58-9804-af9fca68ad80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9547427561056475\n"
     ]
    }
   ],
   "source": [
    "vol5 = compute_volumetric(GTS_nii[4], PTS_nii[4])\n",
    "print(vol5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c77aded2-aad8-4807-beaa-561ddfe82bb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Mean volume:  0.9196774777122358\n",
      "\n",
      "STD volume:  0.050854162445610654\n"
     ]
    }
   ],
   "source": [
    "vol = [0.9802115767442802, 0.893861309773059, 0.9344621027255262, 0.8351096432126663, 0.9547427561056475]\n",
    "#vol = [vol1, vol2, vol3, vol4, vol5]\n",
    "\n",
    "print(\"\\nMean volume: \", np.mean(vol))\n",
    "print(\"\\nSTD volume: \", np.std(vol))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4113da34-71e3-4b09-a5d1-84e9e9acd37d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_confusion_metrics(img1, img2):\n",
    "    \"\"\"\n",
    "    img1 has to be the GROUND TRUTH !\n",
    "    \"\"\"\n",
    "\n",
    "    seg1 = img1.get_fdata()\n",
    "    seg2 = img2.get_fdata()\n",
    "    \n",
    "    labels_1 = np.unique(seg1[seg1 > 0])\n",
    "    labels_2 = np.unique(seg2[seg2 > 0])\n",
    "    my_dict = dict(zip(labels_1, labels_2))\n",
    "    \n",
    "    # Ensure both segmentations have the same shape\n",
    "    assert seg1.shape == seg2.shape, \"The two segmentations must have the same shape.\"\n",
    "    \n",
    "    # Convert the 3D arrays into 1D arrays for easy comparison\n",
    "    seg1_flat = seg1.flatten()\n",
    "    seg2_flat = seg2.flatten()\n",
    "    \n",
    "    for label in labels_1:\n",
    "    \n",
    "        # Compute True Positives, True Negatives, False Positives, and False Negatives\n",
    "        TP = np.sum((seg1_flat == label) & (seg2_flat == my_dict[label]))  # Both true\n",
    "        TN = np.sum((seg1_flat != label) & (seg2_flat != my_dict[label]))  # Both false\n",
    "        FP = np.sum((seg1_flat != label) & (seg2_flat == my_dict[label]))  # False positive: seg2 says 1 but seg1 says 0\n",
    "        FN = np.sum((seg1_flat == label) & (seg2_flat != my_dict[label]))  # False negative: seg2 says 0 but seg1 says 1\n",
    "        \n",
    "        # Total number of voxels\n",
    "        total_voxels = seg1_flat.size\n",
    "        \n",
    "        # Calculate percentages\n",
    "        TP_percent = (TP / total_voxels) * 100\n",
    "        TN_percent = (TN / total_voxels) * 100\n",
    "        FP_percent = (FP / total_voxels) * 100\n",
    "        FN_percent = (FN / total_voxels) * 100\n",
    "        \n",
    "        # Output the results in percentages\n",
    "        print(f'\\nResults for Label: {label}, Total voxels for this label: {np.sum(seg1_flat == label)}')\n",
    "        print(f'True Positives (TP): {TP_percent:.4f}%, TP voxels: {TP}')\n",
    "        print(f'True Negatives (TN): {TN_percent:.4f}%, TN voxels: {TN}')\n",
    "        print(f'False Positives (FP): {FP_percent:.4f}%, FP voxels: {FP}')\n",
    "        print(f'False Negatives (FN): {FN_percent:.4f}%, FN voxels: {FN}')\n",
    "\n",
    "# compute_confusion_metrics(nii_1, nii_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d3e6c759-e9d1-4520-b2da-4726a4d5325f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Mean avg:  2.5389224428466357\n",
      "\n",
      "STD avg:  0.5194738784552952\n"
     ]
    }
   ],
   "source": [
    "def compute_average(img1, img2):\n",
    "    \"\"\"\n",
    "    img1 has to be the GROUND TRUTH !\n",
    "    \"\"\"\n",
    "\n",
    "    # Convert to numpy arrays for easier manipulation\n",
    "    seg1_np = sitk.GetArrayFromImage(img1)\n",
    "    seg2_np = sitk.GetArrayFromImage(img2)\n",
    "    \n",
    "    # Find all unique labels (excluding background)\n",
    "    labels1 = np.unique(seg1_np)\n",
    "    labels2 = np.unique(seg2_np) \n",
    "    \n",
    "    average_surface_distances = []\n",
    "    labels = labels1[1:]\n",
    "    \n",
    "    for label in labels:\n",
    "    \n",
    "        # Extract binary masks for the current label in both segmentations\n",
    "        seg1_mask = (seg1_np == label).astype(np.uint8)\n",
    "        seg2_mask = (seg2_np == label).astype(np.uint8)\n",
    "    \n",
    "        if label == 1:\n",
    "            seg2_mask = (seg2_np == labels2[1]).astype(np.uint8)\n",
    "        if label == 2:\n",
    "            seg2_mask = (seg2_np == labels2[2]).astype(np.uint8)\n",
    "        if label == 3:\n",
    "            seg2_mask = (seg2_np == labels2[3]).astype(np.uint8)\n",
    "        if label == 4:\n",
    "            seg2_mask = (seg2_np == labels2[4]).astype(np.uint8)\n",
    "    \n",
    "        # Convert the masks back to SimpleITK images\n",
    "        seg1_mask_img = sitk.GetImageFromArray(seg1_mask)\n",
    "        seg2_mask_img = sitk.GetImageFromArray(seg2_mask)\n",
    "    \n",
    "        # Compute Average Surface Distance\n",
    "        seg1_surface = sitk.LabelContour(seg1_mask_img)\n",
    "        seg2_surface = sitk.LabelContour(seg2_mask_img)\n",
    "    \n",
    "        surface_distance_filter = sitk.HausdorffDistanceImageFilter()\n",
    "        surface_distance_filter.Execute(seg1_surface, seg2_surface)\n",
    "        avg_distance = surface_distance_filter.GetAverageHausdorffDistance()\n",
    "        average_surface_distances.append(avg_distance)\n",
    "    \n",
    "    # Output results for each label\n",
    "    # for i, label in enumerate(labels):\n",
    "    #     print(f\"Label {label}:\")\n",
    "    #     print(f\"  Average Surface Distance: {average_surface_distances[i]}\")\n",
    "    return np.mean(average_surface_distances)\n",
    "\n",
    "avg = []\n",
    "for gt, pt in zip(GTS, PTS):\n",
    "    avg.append(compute_average(gt, pt))\n",
    "\n",
    "print(\"\\nMean avg: \", np.mean(avg))\n",
    "print(\"\\nSTD avg: \", np.std(avg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41e748aa-818d-444c-a118-2a2e77251f9b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
